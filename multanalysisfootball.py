# -*- coding: utf-8 -*-
"""67gorev.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zKitoEiLenL3vys4nqQQ_VFZ2J-R7Tnn
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
import numpy as np
from statsmodels.multivariate.manova import MANOVA

!pip install FactorAnalyzer

# Load the dataset and immediately filter for CF players
df = pd.read_csv('/content/467allposselfill2.csv')
# Filter for specific leagues and season
leagues = ["England. Premier League", "Germany. Bundesliga", "Spain. LaLiga", "Italy. Serie A", "Turkey. Süper Lig"]
leagues = ["England. Premier League", "Germany. Bundesliga", "Spain. LaLiga", "Italy. Serie A", "Turkey. Süper Lig"]
df = df[
    (df['mainPosition'] == 'cf') &
    (df['competition'].isin(leagues)) &
    (df['season'] == '2023-2024') &
    (df['minutes'] >= 700)
]

# 1. Basic Dataset Information  GEREKSİZ
print("Dataset Shape:", df.shape)
print("\nDataset Info:")
print(df.info())

# 2. Basic Statistics
print("\nBasic Statistics:")
print(df.describe())

# 3. Check for missing values
missing_values = df.isnull().sum()
print("\nMissing Values:")
print(missing_values[missing_values > 0])

# Update numerical columns to include more CF-relevant metrics
numerical_cols = [
    'age', 'height', 'weight', 'minutes',
    'total_goals', 'total_assists', 'total_shots',
    'total_gutto_shots_on_target', 'total_touch_in_box',
    'total_xg_shot', 'total_xg_assist',
    'total_shot_assists', 'total_compa_key_passes_received',
    'total_offsides'
]

# EDA ANALYSIS

# 4. Distribution plots for key striker metrics
plt.figure(figsize=(20, 15))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(4, 4, i)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

#BUNU DA AL
# 5. Correlation Analysis focusing on goal-scoring metrics
goal_scoring_features = [
    'total_goals', 'total_shots', 'total_gutto_shots_on_target',
    'total_touch_in_box', 'total_xg_shot', 'minutes',
    'total_offsides', 'total_shot_assists'
]

passing_features = [
    'opposition_box_passes_score', 'passes_to_final_third_score', 'forward_passes_score',
    'crosses_score', 'progressive_passes_score', 'long_passes_score',
    'through_passes_score', 'opposition_half_passes_score'
]

correlation_matrix = df[goal_scoring_features].corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix of Goal-Scoring Metrics')
plt.show()

# Add striker-specific scatter plots
plt.figure(figsize=(20, 10))
# Goals vs xG
plt.subplot(1, 2, 1)
sns.scatterplot(data=df, x='total_xg_shot', y='total_goals', alpha=0.6)
plt.title('Goals vs Expected Goals')
plt.xlabel('Expected Goals (xG)')
plt.ylabel('Actual Goals')

# Shots vs Goals
plt.subplot(1, 2, 2)
sns.scatterplot(data=df, x='total_shots', y='total_goals', alpha=0.6)
plt.title('Goals vs Total Shots')
plt.xlabel('Total Shots')
plt.ylabel('Total Goals')
plt.tight_layout()
plt.show()

# Efficiency metrics
df['shot_conversion'] = df['total_goals'] / df['total_shots']
df['xg_overperformance'] = df['total_goals'] - df['total_xg_shot']

# Print top performers
print("\nTop 10 Goalscorers:")
print(df.nlargest(10, 'total_goals')[['name', 'total_goals', 'minutes', 'total_shots']])

print("\nTop 10 xG Overperformers:")
print(df.nlargest(10, 'xg_overperformance')[['name', 'total_goals', 'total_xg_shot', 'xg_overperformance']])

print("\nTop 10 Shot Conversion Rates (min. 20 shots):")
print(df[df['total_shots'] >= 20].nlargest(10, 'shot_conversion')[['name', 'shot_conversion', 'total_goals', 'total_shots']])

# 8. Age Distribution Analysis
# Create age groups using quantiles to get approximately equal observations
df['age_group'] = pd.qcut(df['age'], q=3, labels=['Young', 'Mid-Career', 'Veteran'])

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='age_group')
plt.title('Age Distribution by Groups')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.show()

# Print the age ranges for each group
print("\nAge Group Ranges:")
age_ranges = pd.qcut(df['age'], q=3).unique()
for group, age_range in zip(['Young', 'Mid-Career', 'Veteran'], sorted(age_ranges)):
    print(f"{group}: {age_range}")

# 9. Value Analysis
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='age', y='tmValue', alpha=0.5)
plt.title('Player Value vs Age')
plt.xlabel('Age')
plt.ylabel('Transfer Market Value')
plt.show()

# 10. Summary Statistics by Position
position_stats = df.groupby('mainPosition')[['total_goals', 'total_assists', 'total_shots', 'minutes']].mean()
print("\nAverage Statistics by Position:")
print(position_stats)

#BUNU AL
# Check multivariate normality
print("\nChecking Multivariate Normality")

# Select key variables for normality testing
normality_vars = ['total_goals', 'total_shots', 'total_compa_key_passes_received', 'total_xg_shot',
                 'opposition_box_passes_score', 'passes_to_final_third_score',
                 'forward_passes_score', 'total_xg_assist']

# Visual check with Q-Q plots for original data
from scipy import stats
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 10))
for i, var in enumerate(normality_vars, 1):
    plt.subplot(2, 4, i)
    stats.probplot(df[var].dropna(), dist="norm", plot=plt)
    plt.title(f"Q-Q Plot (Original): {var}")
plt.tight_layout()
plt.show()

# Create log-normal transformed variables
# Add small constant to handle zeros
epsilon = 1e-10
df_lognorm = df.copy()
for var in normality_vars:
    # Fix: Add shape parameter 's' for lognorm distribution
    df_lognorm[f'{var}_lognorm'] = np.log1p(df[var] + epsilon)

# Visual check with Q-Q plots for log-normal transformed data
plt.figure(figsize=(15, 10))
for i, var in enumerate(normality_vars, 1):
    plt.subplot(2, 4, i)
    stats.probplot(df_lognorm[f'{var}_lognorm'].dropna(), dist="norm", plot=plt)
    plt.title(f"Q-Q Plot (Log-normal): {var}")
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test for original and log-normal transformed variables
print("\nShapiro-Wilk Test Results:")
print("\nOriginal Data:")
for var in normality_vars:
    stat, p_value = stats.shapiro(df[var].dropna())
    print(f"\n{var}:")
    print(f"Statistic: {stat:.4f}")
    print(f"p-value: {p_value:.4f}")

print("\nLog-normal Transformed Data:")
for var in normality_vars:
    stat, p_value = stats.shapiro(df_lognorm[f'{var}_lognorm'].dropna())
    print(f"\n{var}_lognorm:")
    print(f"Statistic: {stat:.4f}")
    print(f"p-value: {p_value:.4f}")

# Mardia's test for multivariate normality
from scipy.stats import chi2

def mardia_test(data):
    n = len(data)
    p = data.shape[1]

    # Center the data
    data_centered = data - np.mean(data, axis=0)

    # Covariance matrix
    S = np.cov(data_centered, rowvar=False)

    # Calculate Mardia's skewness
    b1p = np.mean([np.sum((data_centered @ np.linalg.inv(S) @ row.values.reshape(-1, 1))**3)
                   for _, row in data_centered.iterrows()])
    skew_stat = (n/6) * b1p
    skew_df = (p * (p + 1) * (p + 2)) / 6
    skew_p_value = 1 - chi2.cdf(skew_stat, skew_df)

    # Calculate Mardia's kurtosis
    b2p = np.mean([np.sum((data_centered @ np.linalg.inv(S) @ row.values.reshape(-1, 1))**2)
                   for _, row in data_centered.iterrows()])
    kurt_stat = (b2p - p * (p + 2)) / np.sqrt(8 * p * (p + 2) / n)
    kurt_p_value = 2 * (1 - stats.norm.cdf(abs(kurt_stat)))

    return skew_stat, skew_p_value, kurt_stat, kurt_p_value

# Perform Mardia's test for original data
print("\nMardia's Multivariate Normality Test (Original Data):")
data_for_mardia = df[normality_vars].dropna()
skew_stat, skew_p, kurt_stat, kurt_p = mardia_test(data_for_mardia)
print(f"Skewness statistic: {skew_stat:.4f}")
print(f"Skewness p-value: {skew_p:.4f}")
print(f"Kurtosis statistic: {kurt_stat:.4f}")
print(f"Kurtosis p-value: {kurt_p:.4f}")

# Perform Mardia's test for log-normal transformed data
print("\nMardia's Multivariate Normality Test (Log-normal Transformed Data):")
lognorm_vars = [f'{var}_lognorm' for var in normality_vars]
data_for_mardia_lognorm = df_lognorm[lognorm_vars].dropna()
skew_stat, skew_p, kurt_stat, kurt_p = mardia_test(data_for_mardia_lognorm)
print(f"Skewness statistic: {skew_stat:.4f}")
print(f"Skewness p-value: {skew_p:.4f}")
print(f"Kurtosis statistic: {kurt_stat:.4f}")
print(f"Kurtosis p-value: {kurt_p:.4f}")

# INFERENCES ABOUT MEAN VECTOR
# Perform one-sample Hotelling's T-squared test using log-transformed variables
# Select numerical variables for the mean vector analysis
mean_vector_vars = [f'{var}_lognorm' for var in [
    'opposition_box_passes_score', 'passes_to_final_third_score', 'forward_passes_score',
    'total_xg_assist', 'total_xg_shot'
]]

# Standardize log-transformed variables using z-score normalization
df_standardized = pd.DataFrame()
for var in mean_vector_vars:
    df_standardized[var] = (df_lognorm[var] - df_lognorm[var].mean()) / df_lognorm[var].std()

# Calculate sample means of standardized variables
sample_means = df_standardized[mean_vector_vars].mean()
print("\nSample Means of Standardized Key Metrics (Log-transformed):")
print(sample_means)

# Check if sample means are close to zero
print("\nAre sample means close to zero?")
print(np.allclose(sample_means, 0, atol=1e-5))

# Calculate covariance matrix of standardized variables
covariance_matrix = df_standardized[mean_vector_vars].cov()
print("\nCovariance Matrix of Standardized Variables (Log-transformed):")
print(covariance_matrix)

# Calculate sample size
n = len(df_standardized)

# Calculate degrees of freedom
p = len(mean_vector_vars)
df_h = p
df_e = n - p

# Hypothesized mean vector (all zeros as null hypothesis)
mu0 = np.zeros(p)

# Calculate Hotelling's T-squared statistic
x_minus_mu = sample_means - mu0
t_squared = n * x_minus_mu.dot(np.linalg.inv(covariance_matrix)).dot(x_minus_mu)

# Convert to F-statistic
f_stat = (df_e / (df_h * (n-1))) * t_squared

# Calculate p-value
p_value = 1 - stats.f.cdf(f_stat, df_h, df_e)

print("\nHotelling's T-squared Test Results (Log-transformed):")
print(f"T-squared statistic: {t_squared:.2f}")
print(f"F-statistic: {f_stat:.2f}")
print(f"Degrees of freedom: {df_h}, {df_e}")
print(f"p-value: {p_value:.4f}")

if p_value < 0.05:
    print("Conclusion: Reject null hypothesis - there is significant evidence that the mean vector differs from zero")
else:
    print("Conclusion: Fail to reject null hypothesis - insufficient evidence that the mean vector differs from zero")

# Calculate confidence intervals for individual means of log-transformed variables
alpha = 0.05
for var in mean_vector_vars:
    ci = stats.t.interval(1-alpha, df_e,
                         loc=df_lognorm[var].mean(),
                         scale=stats.sem(df_lognorm[var]))
    print(f"\n95% Confidence Interval for {var}:")
    print(f"({ci[0]:.2f}, {ci[1]:.2f})")

    #0 çıkyor :()

# Perform MANOVA
print("\nMultivariate Analysis of Variance (MANOVA)")

# Define dependent variables for MANOVA
dependent_vars = ['total_goals', 'total_assists', 'total_xg_assist', 'total_xg_shot']

# Create age groups and competition as independent variables
df['age_group'] = pd.qcut(df['age'], q=3, labels=['Young', 'Mid', 'Experienced'])
df['competition'] = pd.Categorical(df['competition'], categories=['England. Premier League', 'Germany. Bundesliga', 'Italy. Serie A'])

#BUNU AL
# Perform MANOVA for age groups
manova_age = MANOVA.from_formula(f'{" + ".join(dependent_vars)} ~ C(age_group)', data=df)
age_test_results = manova_age.mv_test()

print("\nMANOVA Results for Age Groups:")
print(age_test_results)

# Perform MANOVA for competition
manova_competition = MANOVA.from_formula(f'{" + ".join(dependent_vars)} ~ C(competition)', data=df)
competition_test_results = manova_competition.mv_test()

print("\nMANOVA Results for Competition:")
print(competition_test_results)

# Follow-up univariate ANOVAs for age groups
print("\nFollow-up univariate ANOVAs for Age Groups:")
for dv in dependent_vars:
    age_groups = [group[dv].values for name, group in df.groupby('age_group')]
    f_stat, p_val = stats.f_oneway(*age_groups)
    print(f"\n{dv}:")
    print(f"F-statistic: {f_stat:.4f}")
    print(f"p-value: {p_val:.4f}")

# Follow-up univariate ANOVAs for competition
print("\nFollow-up univariate ANOVAs for Competition:")
for dv in dependent_vars:
    competition_groups = [group[dv].values for name, group in df.groupby('competition')]
    f_stat, p_val = stats.f_oneway(*competition_groups)
    print(f"\n{dv}:")
    print(f"F-statistic: {f_stat:.4f}")
    print(f"p-value: {p_val:.4f}")

# Perform PCA on key striker metrics
print("\nPrincipal Component Analysis (PCA)")

# Standardize the variables for PCA
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Select variables for PCA
pca_vars = ['total_goals', 'total_shots', 'total_gutto_shots_on_target',
            'total_touch_in_box', 'total_xg_shot', 'total_assists',
            'total_xg_assist','total_compa_key_passes_received',
            'opposition_box_passes_score', 'passes_to_final_third_score']

# Standardize the data
X = StandardScaler().fit_transform(df[pca_vars])

# Create and fit PCA with 3 components
pca = PCA(n_components=3)
pca_result = pca.fit_transform(X)

# Calculate explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

#BUNU AL ama result kısmını grafikleştirmemiz lazım
# Print explained variance
print("\nExplained Variance Ratio by Component:")
for i, var in enumerate(explained_variance_ratio):
    print(f"PC{i+1}: {var:.4f}")

print("\nCumulative Explained Variance Ratio:")
for i, var in enumerate(cumulative_variance_ratio):
    print(f"PC1 to PC{i+1}: {var:.4f}")


#Cumulative Explained Variance Ratio:
#PC1 to PC1: 0.7162
#PC1 to PC2: 0.8559
#PC1 to PC3: 0.9029 bu kısmı görselleştirme

import matplotlib.pyplot as plt
import numpy as np

# Data
components = ['PC1 to PC1', 'PC1 to PC2', 'PC1 to PC3']
cumulative_variance_ratio = [0.7162, 0.8559, 0.9029]

# Plotting
plt.figure(figsize=(8, 5))

# Line plot for cumulative explained variance ratio
plt.plot(components, cumulative_variance_ratio, color='red', marker='o', label='Cumulative Explained Variance')
plt.xlabel('Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('Cumulative Explained Variance Ratio')
plt.ylim(0, 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend()

# Show plot
plt.tight_layout()
plt.show()

# Get component loadings for first 3 PCs
loadings = pca.components_
loading_matrix = pd.DataFrame(
    loadings.T,
    columns=['PC1', 'PC2', 'PC3'],
    index=pca_vars
)
print("\nComponent Loadings:")
print(loading_matrix)

# Plot first two principal components
plt.figure(figsize=(10, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('First Two Principal Components')

#çıkarabiliriz

#BU BORANDA BUNU BORANDAN AL
# Add some player names for reference
for i, player in enumerate(df['name']):
    if i % 10 == 0:  # Plot every 10th player name to avoid overcrowding
        plt.annotate(player, (pca_result[i, 0], pca_result[i, 1]))

plt.show()

# Create 3D scatter plot
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(pca_result[:, 0],
                    pca_result[:, 1],
                    pca_result[:, 2],
                    alpha=0.5)

ax.set_xlabel('First Principal Component')
ax.set_ylabel('Second Principal Component')
ax.set_zlabel('Third Principal Component')
plt.title('First Three Principal Components')

# Add some player names in 3D
for i, player in enumerate(df['name']):
    if i % 10 == 0:  # Plot every 10th player name to avoid overcrowding
        ax.text(pca_result[i, 0],
                pca_result[i, 1],
                pca_result[i, 2],
                player)

plt.show()

#üstteki graph cursor'da çıkıyor ona sonra bakarız

########################3
# Principal Components Regression
# First, let's select a target variable (total_goals as an example)
target = df['total_xg_shot']

# Create train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(pca_result, target, test_size=0.2, random_state=42)

# Fit linear regression on the principal components
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
pcr = LinearRegression()
pcr.fit(X_train, y_train)

# Make predictions
y_pred = pcr.predict(X_test)

# Calculate performance metrics
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print("\nPrincipal Components Regression Results:")
print(f"R-squared Score: {r2:.4f}")
print(f"Root Mean Square Error: {rmse:.4f}")

#Net okay NET AL BUNU
# Plot actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Goals')
plt.ylabel('Predicted Goals')
plt.title('Actual vs Predicted Goals using PCR')
plt.show()

# Calculate and plot regression coefficients for each principal component
coefficients = pd.Series(pcr.coef_, index=[f'PC{i+1}' for i in range(len(pcr.coef_))])
plt.figure(figsize=(10, 6))
coefficients.plot(kind='bar')
plt.title('PCR Coefficients')
plt.xlabel('Principal Components')
plt.ylabel('Coefficient Value')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Transform coefficients back to original variables
original_coefficients = np.dot(pca.components_.T, pcr.coef_)
original_coef_df = pd.DataFrame({
    'Variable': pca_vars,
    'Coefficient': original_coefficients
})
original_coef_df = original_coef_df.sort_values('Coefficient', key=abs, ascending=False)

print("\nTop 10 Most Important Original Variables:")
print(original_coef_df.head(10))

#CALISMIYORRRRRRR
# Factor Analysis and Rotation
print("\nFactor Analysis with Rotation")

print("\nFactor Analysis with Rotation")
from factor_analyzer import FactorAnalyzer

# Determine number of factors using eigenvalues > 1 criterion
fa = FactorAnalyzer(rotation=None, n_factors=len(pca_vars))
fa.fit(X)
ev, v = fa.get_eigenvalues()
n_factors = sum(ev > 1)

print(f"\nNumber of factors to extract (eigenvalues > 1): {n_factors}")

# Perform factor analysis with varimax rotation
fa_rotated = FactorAnalyzer(rotation='varimax', n_factors=n_factors)
fa_rotated.fit(X)

# Get factor loadings
factor_loadings = pd.DataFrame(
    fa_rotated.loadings_,
    columns=[f'Factor{i+1}' for i in range(n_factors)],
    index=pca_vars
)

print("\nRotated Factor Loadings:")
print(factor_loadings)

# Calculate variance explained by each factor
variance_explained = fa_rotated.get_factor_variance()[1]
print("\nVariance Explained by Each Factor:")
for i, var in enumerate(variance_explained):
    print(f"Factor {i+1}: {var:.4f}")

  #Loading Plots
#PC1-PC2:
#bunun grapho

#Okay gibi
# Plot factor loadings heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(factor_loadings, annot=True, cmap='coolwarm', center=0)
plt.title('Factor Loadings Heatmap')
plt.tight_layout()
plt.show()

# Calculate and plot communalities
communalities = pd.Series(fa_rotated.get_communalities(), index=pca_vars)
plt.figure(figsize=(10, 6))
communalities.sort_values().plot(kind='bar')
plt.title('Communalities')
plt.xlabel('Variables')
plt.ylabel('Communality')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

########################

#BUNU AL AMA BUNDA ÇİZGİ DEĞİŞTİRİLMESİ LAZIM BAŞLIK LDA ANALYSİS OLMALI
# Correlation Analysis between xG and xA
print("\n### Correlation Analysis between Expected Goals and Expected Assists ###")

# Calculate correlation
correlation = df['total_xg_shot'].corr(df['total_xg_assist'])
print(f"\nCorrelation between xG and xA: {correlation:.4f}")

# Create scatter plot
plt.figure(figsize=(10, 8))

# Add regression line first (to get the line coefficients)
reg = sns.regplot(data=df, x='total_xg_shot', y='total_xg_assist', scatter=False, color='red')
line = reg.get_lines()[0]
slope = line.get_xdata()[1] - line.get_xdata()[0]
intercept = line.get_ydata()[0] - slope * line.get_xdata()[0]

# Create scatter plot with different markers based on position relative to line
for idx, row in df.iterrows():
    predicted_y = slope * row['total_xg_shot'] + intercept
    if row['total_xg_assist'] > predicted_y:
        plt.scatter(row['total_xg_shot'], row['total_xg_assist'], marker='^', color='green')
    else:
        plt.scatter(row['total_xg_shot'], row['total_xg_assist'], marker='o', color='blue')

plt.title('Expected Goals vs Expected Assists')
plt.xlabel('Expected Goals (xG)')
plt.ylabel('Expected Assists (xA)')

# Add player names as annotations for notable points
# Find players with high xG or high xA (top 5 in either metric)
threshold_xg = df['total_xg_shot'].nlargest(5).min()
threshold_xa = df['total_xg_assist'].nlargest(5).min()

for idx, row in df.iterrows():
    if row['total_xg_shot'] >= threshold_xg or row['total_xg_assist'] >= threshold_xa:
        plt.annotate(row['name'],
                    (row['total_xg_shot'], row['total_xg_assist']),
                    xytext=(5, 5), textcoords='offset points')

plt.tight_layout()
plt.show()

# Perform statistical test (Pearson's correlation test)
correlation_stat, p_value = stats.pearsonr(df['total_xg_shot'], df['total_xg_assist'])
print(f"\nPearson Correlation Test Results:")
print(f"Correlation coefficient: {correlation_stat:.4f}")
print(f"P-value: {p_value:.4f}")

# Calculate summary statistics
print("\nSummary Statistics:")
print("\nExpected Goals (xG):")
print(df['total_xg_shot'].describe())
print("\nExpected Assists (xA):")
print(df['total_xg_assist'].describe())

# DICS
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Perform Linear Discriminant Analysis (LDA)
print("\n### Linear Discriminant Analysis ###")

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[pca_vars])

# Create binary classifications based on goals (you can adjust the threshold)
median_goals = df['total_goals'].median()
y = (df['total_goals'] > median_goals).astype(int)
print(f"\nUsing median goals ({median_goals}) as threshold for classification")
print(f"Number of players above median: {sum(y == 1)}")
print(f"Number of players below median: {sum(y == 0)}")

# Fit LDA
lda = LinearDiscriminantAnalysis()
X_lda = lda.fit_transform(X_scaled, y)

# Print explained variance ratio
print("\nExplained variance ratio:", lda.explained_variance_ratio_)

# Get the coefficients
lda_coefficients = pd.DataFrame(
    lda.coef_.T,
    columns=['LD1'],
    index=pca_vars
)
print("\nLDA Coefficients:")
print(lda_coefficients)

#We should seperate more accurate and we should define as "poacher" and "second striker". Also we should define skore according to LDA projection to better understand what kind of striker is the new player
#xG and xA must be added
#https://medium.com/@mertcengiz2100/linear-discriminant-analysis-do%C4%9Frusal-diskriminant-analizi-nedir-cc1e7fb8c129
# Plot LDA projection
plt.figure(figsize=(10, 6))
for label in [0, 1]:
    mask = y == label
    plt.hist(X_lda[mask], alpha=0.5, label=f'Class {label}', bins=20)
plt.xlabel('LD1')
plt.ylabel('Count')
plt.title('LDA Projection')
plt.legend(['Below Median Goals', 'Above Median Goals'])
plt.tight_layout()
plt.show()

# Plot coefficient importance
plt.figure(figsize=(10, 6))
lda_coefficients['LD1'].abs().sort_values().plot(kind='bar')
plt.title('Absolute LDA Coefficients')
plt.xlabel('Variables')
plt.ylabel('Coefficient Magnitude')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# K-means Clustering
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import dendrogram, linkage
print("\n### K-means Clustering Analysis ###")

# Select features for clustering
cluster_vars = ['total_goals', 'total_assists', 'total_shots', 'total_xg_shot']
X_cluster = StandardScaler().fit_transform(df[cluster_vars])



# Compute pairwise Euclidean distances
distances = pdist(X_cluster, metric='euclidean')
distance_matrix = squareform(distances)

# Perform hierarchical clustering with complete linkage
linkage_matrix = linkage(distances, method='complete')

# Plot dendrogram
plt.figure(figsize=(12, 8))
dendrogram(linkage_matrix)
plt.title('Hierarchical Clustering Dendrogram (Complete Linkage)')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()



# Determine optimal number of clusters using silhouette score
silhouette_scores = []
K = range(2, 8)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_cluster)
    score = silhouette_score(X_cluster, kmeans.labels_)
    silhouette_scores.append(score)

#BUNU AL
# Plot silhouette scores
plt.figure(figsize=(10, 6))
plt.plot(K, silhouette_scores, 'bo-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score vs Number of Clusters')
plt.show()

# Get optimal k
optimal_k = K[np.argmax(silhouette_scores)]
print(f"\nOptimal number of clusters: {optimal_k}")

# Perform k-means clustering with optimal k
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
df['Cluster'] = kmeans.fit_predict(X_cluster)

# Analyze clusters
cluster_means = df.groupby('Cluster')[cluster_vars].mean()
print("\nCluster Means:")
print(cluster_means)

#BUNU AL
# Visualize clusters using first two principal components
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_cluster)

plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['Cluster'], cmap='viridis')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('Cluster Visualization using PCA')
plt.colorbar(scatter, label='Cluster')

# Add some player names for reference
for i, player in enumerate(df['name']):
    if i % 10 == 0:  # Plot every 10th player name to avoid overcrowding
        plt.annotate(player, (X_pca[i, 0], X_pca[i, 1]))

plt.tight_layout()
plt.show()

# Print top players in each cluster
print("\nTop Players in Each Cluster:")
for cluster in range(optimal_k):
    print(f"\nCluster {cluster}:")
    cluster_players = df[df['Cluster'] == cluster].nlargest(5, 'total_goals')[['name', 'total_goals', 'total_assists', 'total_shots']]
    print(cluster_players)

# Canonical Correlation Analysis
print("\n### Canonical Correlation Analysis ###")

from sklearn.cross_decomposition import CCA

# Define two sets of variables for canonical correlation
# Set 1: Goal-scoring related variables
X1 = df[['total_goals', 'total_shots', 'total_gutto_shots_on_target', 'total_xg_shot']]

# Set 2: Passing/Creation related variables
X2 = df[['total_assists', 'total_shot_assists', 'total_xg_assist', 'total_compa_key_passes_received']]

# Standardize both sets
X1_scaled = StandardScaler().fit_transform(X1)
X2_scaled = StandardScaler().fit_transform(X2)

# Perform CCA
n_components = min(X1_scaled.shape[1], X2_scaled.shape[1])
cca = CCA(n_components=n_components)
X1_cca, X2_cca = cca.fit_transform(X1_scaled, X2_scaled)

# Calculate correlations between canonical variates
correlations = [np.corrcoef(X1_cca[:, i], X2_cca[:, i])[0, 1] for i in range(n_components)]

# Print canonical correlations
print("\nCanonical Correlations:")
for i, corr in enumerate(correlations):
    print(f"Canonical Correlation {i+1}: {corr:.4f}")

# Plot canonical correlations
plt.figure(figsize=(10, 6))
plt.bar(range(1, len(correlations) + 1), correlations)
plt.xlabel('Canonical Variate')
plt.ylabel('Correlation')
plt.title('Canonical Correlations')
plt.show()

# Scatter plot of first canonical variates
plt.figure(figsize=(10, 6))
plt.scatter(X1_cca[:, 0], X2_cca[:, 0], alpha=0.5)
plt.xlabel('First Canonical Variate (Goal-scoring)')
plt.ylabel('First Canonical Variate (Creation)')
plt.title('First Canonical Variates Relationship')

# Add some player names for reference
for i, player in enumerate(df['name']):
    if i % 10 == 0:  # Plot every 10th player name to avoid overcrowding
        plt.annotate(player, (X1_cca[i, 0], X2_cca[i, 0]))

plt.tight_layout()
plt.show()

# Calculate and print canonical loadings
X1_loadings = np.corrcoef(X1_scaled, X1_cca, rowvar=False)[:X1.shape[1], -n_components:]
X2_loadings = np.corrcoef(X2_scaled, X2_cca, rowvar=False)[:X2.shape[1], -n_components:]

# Create DataFrames for loadings
X1_loadings_df = pd.DataFrame(X1_loadings,
                             index=X1.columns,
                             columns=[f'CV{i+1}' for i in range(n_components)])
X2_loadings_df = pd.DataFrame(X2_loadings,
                             index=X2.columns,
                             columns=[f'CV{i+1}' for i in range(n_components)])

print("\nCanonical Loadings for Goal-scoring Variables:")
print(X1_loadings_df)
print("\nCanonical Loadings for Creation Variables:")
print(X2_loadings_df)

#BUNU AL
# Heatmap of canonical loadings
plt.figure(figsize=(12, 8))
plt.subplot(1, 2, 1)
sns.heatmap(X1_loadings_df, annot=True, cmap='coolwarm', center=0)
plt.title('Goal-scoring Variables Loadings')

plt.subplot(1, 2, 2)
sns.heatmap(X2_loadings_df, annot=True, cmap='coolwarm', center=0)
plt.title('Creation Variables Loadings')

plt.tight_layout()
plt.show()



